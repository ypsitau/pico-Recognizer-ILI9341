{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ypsitau/pico-Recognizer-ILI9341/blob/main/pico-Recognizer-ILI9341.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKteFZbTa31S"
      },
      "source": [
        "# Create Recognizer Model from MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYRyrDE03gN8"
      },
      "source": [
        "The following script mounts Google Drive and specifies the TensorFlow dataset to it. It will save time of downloading the next time you use the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDr_tE1ns6aV",
        "outputId": "9e65a932-445a-491b-930b-adf7a3f81e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.environ['TFDS_DATA_DIR'] = '/content/drive/MyDrive/tensorflow_datasets'\n",
        "os.makedirs(os.environ['TFDS_DATA_DIR'], exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L60yChUaa08o"
      },
      "source": [
        "Running the following script creates TfLite model files. They will be automatically downloaded to your remote PC after the training.\n",
        "\n",
        "|TfLite Model File|Recognizable Characters|\n",
        "|----|----|\n",
        "|`Recognizer-emnist-mnist-binary.tflite`|Digits (0-9)|\n",
        "|`Recognizer-emnist-letters-binary.tflite`|Upper letters (A-Z)|\n",
        "|`Recognizer-emnist-balanced-binary.tflite`|Digits (0-9), upper letters (A-Z), and lower letters (a-z)|\n",
        "|`Recognizer-emnist-bymerge-binary.tflite`|Digits (0-9), upper letters (A-Z), and lower letters (a-z)|\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuqS1X-0Dg38"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import google.colab\n",
        "\n",
        "def generate_model(dataset_name: str, config_name: str | None, epochs: int, binary_image: bool) -> str:\n",
        "    need_transpose = (dataset_name == 'emnist')\n",
        "    label_offset = 1 if dataset_name == 'emnist' and config_name == 'letters' else 0\n",
        "    dataset_fullname = dataset_name + ('/' + config_name if config_name else '')\n",
        "    filename = (f\"Recognizer-{dataset_name}\"\n",
        "        f\"{'-' + config_name if config_name else ''}\"\n",
        "        f\"{'-binary' if binary_image else ''}.tflite\")\n",
        "    (dataset_train, dataset_test), dataset_info = tfds.load( # tf.data.Dataset\n",
        "        dataset_fullname,\n",
        "        split=['train', 'test'],\n",
        "        as_supervised=True, # each data element is a tuple (image, label)\n",
        "        with_info=True,\n",
        "    )\n",
        "    num_classes = dataset_info.features['label'].num_classes - label_offset\n",
        "    #---------------------------------------------------------------------------\n",
        "    print(f\"{dataset_fullname}: {num_classes} classes\")\n",
        "    def preprocess(image: tf.Tensor, label: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
        "        if need_transpose:\n",
        "            image = tf.transpose(image, perm=[1, 0, 2])\n",
        "        image = tf.cast(image > 127, tf.float32) if binary_image else tf.cast(image, tf.float32) / 255.0\n",
        "        label = label - label_offset\n",
        "        return image, label\n",
        "    dataset_train = dataset_train.map(preprocess).cache().shuffle(10000).batch(64).prefetch(tf.data.AUTOTUNE)\n",
        "    dataset_test = dataset_test.map(preprocess).batch(64).cache().prefetch(tf.data.AUTOTUNE)\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(dataset_train, epochs=epochs, validation_data=dataset_test)\n",
        "    #---------------------------------------------------------------------------\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    def representative_dataset_generator():\n",
        "        for image, label in dataset_test.take(100):\n",
        "            yield [image]\n",
        "    converter.representative_dataset = representative_dataset_generator\n",
        "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "    converter.inference_input_type = tf.int8\n",
        "    converter.inference_output_type = tf.int8\n",
        "    model_tflite = converter.convert()\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(model_tflite)\n",
        "    return filename\n",
        "\n",
        "filenames = []\n",
        "filenames.append(generate_model('emnist', 'mnist', 5, True))     # 0-9\n",
        "filenames.append(generate_model('emnist', 'letters', 10, True))  # A-Z, 0-9\n",
        "filenames.append(generate_model('emnist', 'balanced', 15, True)) # 0-9, A-Z, a, b, d, e, f, g, h, n, q, r, t\n",
        "filenames.append(generate_model('emnist', 'bymerge', 15, True))  # 0-9, A-Z, a, b, d, e, f, g, h, n, q, r, t\n",
        "#filenames.append(generate_model('kmnist', None, 10, True))       # 10 characters of hiragana\n",
        "for filename in filenames:\n",
        "    google.colab.files.download(filename)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}